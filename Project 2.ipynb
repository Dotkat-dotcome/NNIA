{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "(X_trainval_Original, Y_trainval_Original), (X_test_Original, Y_test_Original) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "X_trainval_Original = np.reshape(X_trainval_Original, (X_trainval_Original.shape[0],  X_trainval_Original.shape[1] *  X_trainval_Original.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "X_test_Original = np.reshape(X_test_Original, (X_test_Original.shape[0],  X_test_Original.shape[1] *  X_test_Original.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    # TODO: Implement\n",
    "    X_trainval_normalized = np.zeros(X_trainval.shape)\n",
    "    X_trainval_mean = np.mean(X_trainval, axis = 0) \n",
    "    #print(X_trainval_mean)\n",
    "    X_trainval_std  = np.std(X_trainval, axis = 0)\n",
    "    #print(X_trainval_std)\n",
    "    X_trainval_normalized = (X_trainval_normalized - np.transpose(X_trainval_mean)) / np.transpose(X_trainval_std)\n",
    "    \n",
    "    X_test_normalized = np.zeros(X_test.shape)\n",
    "    X_test_mean = np.mean(X_test, axis = 0) \n",
    "    X_test_std  = np.std(X_test, axis = 0)\n",
    "    X_test_normalized = (X_test_normalized - np.transpose(X_test_mean)) / np.transpose(X_test_std)\n",
    "    \n",
    "    \n",
    "    return X_trainval_normalized, X_test_normalized\n",
    "\n",
    "def normalize_zscore(X_trainval, X_test):\n",
    "    from scipy import stats\n",
    "    X_trainval_normalized = stats.zscore(X_trainval)\n",
    "    X_test_normalized = stats.zscore(X_test)\n",
    "    return X_trainval_normalized,X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168\n",
      " 133  16   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  49\n",
      " 136 219 216 228 236 255 255 255 255 217 215 254 231 160  45   0   0   0\n",
      "   0   0   0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204\n",
      " 202 201 201 201 209 218 224 164   0   0   0   0   0   0   0   0   0 188\n",
      " 219 200 198 202 198 199 199 201 196 198 198 200 200 200 200 201 200 225\n",
      "  41   0   0   0   0   0   0   0  51 219 199 203 203 212 238 248 250 245\n",
      " 249 246 247 252 248 235 207 203 203 222 140   0   0   0   0   0   0   0\n",
      " 116 226 206 204 207 204 101  75  47  73  48  50  45  51  63 113 222 202\n",
      " 206 220 224   0   0   0   0   0   0   0 200 222 209 203 215 200   0  70\n",
      "  98   0 103  59  68  71  49   0 219 206 214 210 250  38   0   0   0   0\n",
      "   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255 205   0\n",
      " 215 217 214 208 220  95   0   0   0   0   0  45 226 214 214 215 224 205\n",
      "   0  42  35  60  16  17  12  13  70   0 189 216 212 206 212 156   0   0\n",
      "   0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76\n",
      "  92  87 206 207 222 213 219 208   0   0   0   0   0 106 187 223 237 248\n",
      " 211 198 252 250 248 245 248 252 253 250 252 239 201 212 225 215 193 113\n",
      "   0   0   0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200\n",
      " 200 201 203 195 210 165   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  47 225 192 214 203 206 204 204 205 206 204 212 197 218 107   0   0\n",
      "   0   0   0   0   0   0   0   0   1   6   0  46 212 195 212 202 206 205\n",
      " 204 205 206 204 212 200 218  91   0   3   1   0   0   0   0   0   0   0\n",
      "   0   1   0  11 197 199 205 202 205 206 204 205 207 204 205 205 218  77\n",
      "   0   5   0   0   0   0   0   0   0   0   0   3   0   2 191 198 201 205\n",
      " 206 205 205 206 209 206 199 209 219  74   0   5   0   0   0   0   0   0\n",
      "   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208 198 207\n",
      " 221  72   0   4   0   0   0   0   0   0   0   0   0   2   0   0 215 198\n",
      " 203 206 208 205 207 207 210 208 200 202 222  75   0   4   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206\n",
      " 205 198 221  80   0   3   0   0   0   0   0   0   0   0   0   1   0   0\n",
      " 204 201 205 208 207 205 211 205 210 210 209 195 221  96   0   3   0   0\n",
      "   0   0   0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206\n",
      " 210 209 210 194 217 105   0   2   0   0   0   0   0   0   0   0   0   1\n",
      "   0   0 204 204 205 208 207 205 215 207 210 208 211 193 213 115   0   2\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0 204 207 207 208 206 206\n",
      " 215 210 210 207 212 195 210 118   0   2   0   0   0   0   0   0   0   0\n",
      "   0   1   0   0 198 208 208 208 204 207 212 212 210 207 211 196 207 121\n",
      "   0   1   0   0   0   0   0   0   0   0   0   1   0   0 198 210 207 208\n",
      " 206 209 213 212 211 207 210 197 207 124   0   1   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201 205 197\n",
      " 206 127   0   0   0   0   0   0   0   0   0   0   0   0   0   0 188 221\n",
      " 214 234 236 238 244 244 244 240 243 214 224 162   0   2   0   0   0   0\n",
      "   0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121\n",
      " 119 114 130  76   0   0   0   0   0   0]\n",
      "[-0.00864371 -0.02322333 -0.03917806 -0.04132172 -0.05764574  0.10043738\n",
      " -0.09887846 -0.15665259 -0.23908034 -0.37782706  0.13926149  2.147499\n",
      "  0.74160291  0.04606765 -0.03740187 -0.15683454  0.52153311  2.02646418\n",
      "  2.06400752  0.17186363 -0.20335123 -0.14022098 -0.10981155 -0.09222425\n",
      " -0.06827667 -0.05051678 -0.0332743  -0.01362235 -0.01257211 -0.0178614\n",
      " -0.03304933  0.09771095 -0.07809375 -0.13108968 -0.24379931  0.65301192\n",
      "  1.58308149  2.01939868  1.42632319  1.2800209   1.18635873  1.39919806\n",
      "  1.42658452  1.35358907  1.42282872  1.29796131  1.55849899  2.7510352\n",
      "  3.45523366  3.39633868  1.29986818 -0.14894698 -0.11854528 -0.09005845\n",
      " -0.05847776 -0.02894636 -0.01552501 -0.02550164 -0.04603359 -0.07369479\n",
      " -0.12885574  0.12799262  2.67869613  2.69275068  2.21215338  1.62007873\n",
      "  1.15535897  1.02316588  0.94620564  0.95677597  1.09860772  0.99046698\n",
      "  1.03108272  1.08252418  1.21404753  1.61764531  2.1397708   2.72800813\n",
      "  3.92100506  4.27180671 -0.16990354 -0.12987352 -0.08776806 -0.05104194\n",
      " -0.02528823 -0.04090184 -0.06615758 -0.10373973 -0.21458636  3.19506612\n",
      "  2.67912686  1.9445486   1.56172728  1.30612952  1.04688335  1.03946423\n",
      "  1.00154198  0.99704786  0.93308259  0.96204144  0.98989111  1.03920653\n",
      "  1.0760138   1.32346581  1.62531822  1.98323053  2.47325819  4.06987168\n",
      "  0.87060904 -0.17250812 -0.11909081 -0.07178061 -0.0387036  -0.06236771\n",
      " -0.09134164 -0.14997281  0.98907021  3.04624546  1.9657339   1.69037674\n",
      "  1.48171421  1.35711004  1.4668464   1.58323958  1.5674516   1.48097461\n",
      "  1.50104067  1.47385662  1.50957578  1.58610423  1.56610752  1.59673458\n",
      "  1.48101209  1.64753483  2.05630391  3.12513421  2.66144701 -0.22820271\n",
      " -0.1631947  -0.09778169 -0.05277385 -0.08553474 -0.12320564 -0.19988408\n",
      "  1.99351858  2.68024432  1.81430677  1.58026941  1.44952842  1.21979286\n",
      " -0.0160353  -0.30759678 -0.62058731 -0.36647688 -0.6763337  -0.67985152\n",
      " -0.76839074 -0.70967482 -0.5427409   0.13319924  1.51700427  1.44474712\n",
      "  1.76830738  2.54535182  3.4816331  -0.29583093 -0.21582075 -0.12499384\n",
      " -0.06747663 -0.10782708 -0.15469901 -0.24553004  3.09287103  2.34171695\n",
      "  1.70518388  1.46619532  1.47851416  1.15497391 -1.12900065 -0.3799452\n",
      " -0.08069667 -1.19578601 -0.11618302 -0.64171948 -0.58637304 -0.56461381\n",
      " -0.77651127 -1.21919805  1.37550495  1.34490777  1.6494745   2.07530403\n",
      "  3.28075462  0.33086116 -0.26152519 -0.14734593 -0.08522363 -0.13066469\n",
      " -0.18455691 -0.28514856  3.49628705  2.10397041  1.63194029  1.44743188\n",
      "  1.42891521  1.30446745 -1.14641003  1.64612681  1.50181984  0.30798268\n",
      "  1.56031806  0.58438463  1.47242006  1.53244051  0.96640221 -1.29817917\n",
      "  1.22991024  1.33998346  1.49283425  1.81788679  2.46197417  1.0813786\n",
      " -0.30379662 -0.16430184 -0.0964996  -0.1496046  -0.21081783  0.63674054\n",
      "  2.93635966  1.93617751  1.58822309  1.41281893  1.46588499  1.19581078\n",
      " -1.15960337 -0.73613504 -0.83541059 -0.60759511 -1.16578846 -1.31752165\n",
      " -1.50375829 -1.50793382 -0.68271243 -1.36313462  0.84741832  1.21855364\n",
      "  1.34696715  1.61253019  2.10250161  1.74642107 -0.34725508 -0.18639255\n",
      " -0.1064871  -0.16643    -0.22885508  2.93983848  2.91809562  1.84807623\n",
      "  1.51060475  1.4153177   1.33356701  1.13884561 -0.60758684 -0.44350217\n",
      " -0.27957058 -0.28372025 -0.45757313 -0.77785594 -0.98336621 -0.88296673\n",
      " -0.50115085 -0.41044731  0.96015849  1.05703457  1.36923012  1.56316537\n",
      "  1.99765429  2.18796242 -0.39767577 -0.21485073 -0.11664948 -0.17992321\n",
      " -0.23713156  1.73954039  2.17400004  1.91020613  1.78860551  1.70092123\n",
      "  1.26308019  1.08993946  1.56323513  1.52759772  1.44687591  1.41935621\n",
      "  1.463564    1.36063724  1.41306697  1.38185297  1.40858756  1.30816499\n",
      "  0.87362241  1.07863803  1.34950821  1.49583308  1.55691235  0.77528176\n",
      " -0.45419206 -0.24107042 -0.13007545 -0.19216759 -0.2383067  -0.33531038\n",
      " -0.49930334 -0.47979018 -0.18578108  0.79012533  1.37352172  1.00888498\n",
      "  1.02592849  0.79285185  0.76985086  0.81964887  0.83260925  0.65716379\n",
      "  0.66138469  0.67913727  0.74691346  0.7530352   0.93851886  0.54902244\n",
      " -1.10009082 -0.93652258 -0.75211107 -0.65323847 -0.50652292 -0.26971893\n",
      " -0.14427641 -0.20364964 -0.24589361 -0.33860285 -0.50433937 -0.67324005\n",
      " -0.77553613 -0.39069416  1.38953185  0.92310718  1.00813818  0.85123384\n",
      "  0.83303838  0.8463889   0.8552421   0.71259269  0.72258153  0.69609713\n",
      "  0.82621231  0.73391588  0.98483469 -0.12144326 -1.14484395 -0.97448675\n",
      " -0.77407718 -0.66866669 -0.5442604  -0.30315251 -0.15285127 -0.21858798\n",
      " -0.26390838 -0.36016049 -0.52329123 -0.64308936 -0.82695644 -0.4619557\n",
      "  1.20221686  0.8860946   0.94799802  0.80540571  0.79646512  0.82455804\n",
      "  0.83267049  0.70431409  0.70461907  0.6845449   0.81143077  0.77054278\n",
      "  0.98708547 -0.3102184  -1.15960646 -0.96459866 -0.77658507 -0.67626391\n",
      " -0.56981605 -0.33483839 -0.18233383 -0.27265711 -0.32529278 -0.42629759\n",
      " -0.61567528 -0.78416821 -0.89830472 -0.88364152  1.02858615  0.89167007\n",
      "  0.82877646  0.75406486  0.75069205  0.82063514  0.82129053  0.70469348\n",
      "  0.70431716  0.67793031  0.70236188  0.84147044  1.02084119 -0.45155092\n",
      " -1.16120864 -0.95708968 -0.80195601 -0.68451817 -0.59464067 -0.36438474\n",
      " -0.24963437 -0.35367619 -0.39851828 -0.49565208 -0.68969449 -0.82300843\n",
      " -0.94241852 -1.01404936  0.94436786  0.82222149  0.73245913  0.75019794\n",
      "  0.73861306  0.78906018  0.81666262  0.7111878   0.71535782  0.69142442\n",
      "  0.59646103  0.87953239  1.04966026 -0.48619737 -1.16686619 -0.97453795\n",
      " -0.82477563 -0.69831597 -0.61644166 -0.39051021 -0.31045199 -0.40313977\n",
      " -0.44656661 -0.54777665 -0.74737852 -0.88221692 -0.99154697 -1.08682093\n",
      "  0.86653058  0.74862698  0.67713848  0.74379822  0.72921869  0.75215806\n",
      "  0.81296384  0.71529109  0.71372096  0.70984056  0.57375007  0.84590357\n",
      "  1.09816337 -0.49000309 -1.16051891 -0.99532484 -0.84257293 -0.7085525\n",
      " -0.62634245 -0.41046364 -0.32255935 -0.432812   -0.48626339 -0.59148119\n",
      " -0.79334195 -0.91688785 -1.02554414 -1.12302858  1.14931042  0.72667183\n",
      "  0.70085658  0.71313991  0.73483581  0.76496404  0.81353151  0.73089975\n",
      "  0.72417954  0.72723958  0.63768824  0.81941214  1.18391988 -0.39968199\n",
      " -1.11708743 -0.97276042 -0.83094961 -0.69054533 -0.60882709 -0.4070712\n",
      " -0.2939847  -0.43126666 -0.50227154 -0.61373956 -0.81562357 -0.93837718\n",
      " -1.03325008 -1.13031408  1.10641524  0.72499496  0.79268632  0.72017903\n",
      "  0.76110457  0.80115452  0.84952293  0.76064026  0.76200921  0.72412377\n",
      "  0.74765554  0.8137316   1.24401053 -0.28849713 -1.07315473 -0.95356724\n",
      " -0.81183299 -0.66772097 -0.58494182 -0.39644479 -0.27315662 -0.40160299\n",
      " -0.47716524 -0.59814417 -0.80241399 -0.91557774 -1.00958067 -1.10699302\n",
      "  1.01980041  0.78345412  0.75514086  0.75678343  0.73983304  0.80163011\n",
      "  0.90331417  0.7588568   0.76283818  0.79649619  0.83578951  0.81656345\n",
      "  1.31073575 -0.06217962 -1.03149532 -0.92175025 -0.78806147 -0.64021416\n",
      " -0.54650465 -0.35879367 -0.26333929 -0.38095444 -0.44983468 -0.56649982\n",
      " -0.7669243  -0.87581427 -0.96634349 -1.05607886  1.00821465  0.80470971\n",
      "  0.77146399  0.78056888  0.75828626  0.82380487  0.95578031  0.80584289\n",
      "  0.79524481  0.81996076  0.90816175  0.87018863  1.35186961  0.12510795\n",
      " -0.9569395  -0.86807605 -0.73392461 -0.58070806 -0.4779644  -0.30347067\n",
      " -0.24718295 -0.35301119 -0.42185521 -0.53942912 -0.73259351 -0.82947781\n",
      " -0.90850324 -0.98165477  1.05006567  0.86763554  0.79512394  0.78162426\n",
      "  0.7788666   0.84803768  1.00751785  0.86036093  0.83904356  0.86192464\n",
      "  0.98816003  0.9333099   1.39456511  0.32660648 -0.8774839  -0.79529429\n",
      " -0.67016371 -0.52066581 -0.42336625 -0.26967668 -0.2185877  -0.31187625\n",
      " -0.37966098 -0.49814632 -0.68568334 -0.78627384 -0.85051084 -0.91408698\n",
      "  1.10124653  0.95468812  0.86811792  0.82167966  0.81199517  0.90729655\n",
      "  1.06650406  0.96267533  0.90820612  0.92140377  1.08219215  1.0292835\n",
      "  1.46677282  0.46256213 -0.81067074 -0.73209298 -0.61572986 -0.46653942\n",
      " -0.37072961 -0.2298913  -0.18508053 -0.26382263 -0.33358173 -0.45386163\n",
      " -0.6345651  -0.7150227  -0.7821498  -0.83216097  1.11734069  1.04055638\n",
      "  0.94584744  0.87931349  0.84851538  0.98083564  1.10326016  1.06029893\n",
      "  0.98175296  0.99037633  1.14702104  1.11721047  1.55295725  0.61202603\n",
      " -0.74334804 -0.67941556 -0.55761867 -0.40750986 -0.3129627  -0.18758767\n",
      " -0.1398625  -0.21147602 -0.28346082 -0.40690103 -0.58467405 -0.6612695\n",
      " -0.7231625  -0.75811083  1.23089587  1.16279413  1.02518281  0.95553193\n",
      "  0.94815274  1.08331547  1.20787148  1.15263134  1.07361057  1.06398768\n",
      "  1.22508184  1.23258421  1.69159145  0.77120354 -0.68991488 -0.6261985\n",
      " -0.50685478 -0.35493142 -0.26152754 -0.15049193 -0.10000649 -0.16163528\n",
      " -0.23385237 -0.35825792 -0.53327855 -0.61989942 -0.66331587 -0.67806815\n",
      "  1.10015343  1.30189608  1.11726319  1.0048267   0.99814378  1.16095081\n",
      "  1.28412799  1.2125612   1.11745503  1.10922119  1.28887752  1.36793228\n",
      "  1.85940941  0.94825274 -0.63714816 -0.58255601 -0.45287254 -0.29742739\n",
      " -0.20892432 -0.11137299 -0.05617561 -0.10437285 -0.17772606 -0.30551127\n",
      " -0.47483839 -0.55913847 -0.59387784 -0.58784101  1.46102862  1.5277498\n",
      "  1.2640171   1.34074472  1.36367254  1.50414932  1.67503896  1.59959619\n",
      "  1.50025133  1.49626308  1.72089904  1.70786872  2.27217824  1.52866228\n",
      " -0.57433676 -0.49358714 -0.39816627 -0.24255913 -0.1551864  -0.07603711\n",
      " -0.02515919 -0.04823348 -0.10154972 -0.20911958 -0.3404624  -0.40530457\n",
      " -0.43642851 -0.4054615   2.42587141  2.10399869  1.42140542  1.21799297\n",
      "  1.18034752  1.34143526  1.24178558  1.1807316   1.09050729  1.12750025\n",
      "  1.46698394  1.85739499  2.69774137  1.32468229 -0.44135849 -0.39662626\n",
      " -0.28815575 -0.15681128 -0.08967308 -0.03414729]\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "print(X_trainval_Original[1])\n",
    "#print(X_trainval[5995:])\n",
    "#X_trainval, X_test = data_normalization(X_trainval_Original, X_test_Original)\n",
    "X_trainval, X_test = normalize_zscore(X_trainval_Original, X_test_Original)\n",
    "print(X_trainval[1])\n",
    "#print(X_trainval_zscore[1])\n",
    "#print(X_trainval[5995:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    \"\"\"\n",
    "    #TODO: Implement\n",
    "    \n",
    "    from random import seed\n",
    "    from random import randrange\n",
    "\n",
    "    \n",
    "    X_trainval_quotient = X_trainval.shape[0] // K\n",
    "    X_trainval_rest = X_trainval.shape[0] % K\n",
    "    #print(\"X_quotient\", X_trainval_quotient)\n",
    "    #print(\"X_rest\", X_trainval_rest)\n",
    "    first_time = True\n",
    "    #print(\"i is \", i)\n",
    "    \n",
    "    for j in range(K):\n",
    "        \n",
    "        if i == j:\n",
    "            if (i+1) != K:\n",
    "                X_val = X_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (i) + X_trainval_quotient)]\n",
    "                Y_val = Y_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (i) + X_trainval_quotient)]\n",
    "            else:\n",
    "                X_val = X_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (i) + X_trainval_quotient + X_trainval_rest + 1)]\n",
    "                Y_val = Y_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (i) + X_trainval_quotient + X_trainval_rest + 1)]\n",
    "        else:\n",
    "            if (j+1) != K:\n",
    "                if first_time:\n",
    "                    X_train = X_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient)]\n",
    "                    Y_train = Y_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient)]\n",
    "                    first_time = False\n",
    "                    #print(\"hello1\", X_train.shape)\n",
    "                else:\n",
    "                    X_temp = X_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient)]\n",
    "                    Y_temp = Y_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient)]\n",
    "                    \n",
    "                    X_train = np.append(X_train, X_temp, axis=0)\n",
    "                    #print(\"hello2\", X_train.shape)\n",
    "                    Y_train = np.append(Y_train, Y_temp)\n",
    "            else:\n",
    "                if first_time:\n",
    "                    X_train = X_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_rest + 1)]\n",
    "                    Y_train = Y_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_rest + 1)]\n",
    "                    first_time = False\n",
    "                    #print(\"hello3\", X_train.shape)\n",
    "                else:\n",
    "                    X_temp = X_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient + X_trainval_rest + 1)]\n",
    "                    Y_temp = Y_trainval[(X_trainval_quotient * (j)) : (X_trainval_quotient * (j) + X_trainval_quotient + X_trainval_rest + 1)]\n",
    "                    #print(\"hello4X_temp\", X_temp.shape)\n",
    "                    X_train = np.append(X_train, X_temp, axis=0)\n",
    "                    #print(\"hello4\", X_train.shape)\n",
    "                    Y_train = np.append(Y_train, Y_temp)\n",
    "   \n",
    "    #print(X_train.shape)\n",
    "    #print(X_val.shape)\n",
    "    \n",
    "    #print(Y_train.shape)\n",
    "    #print(Y_val.shape)\n",
    "    \n",
    "#     i = i + 1\n",
    "#     if i != K: \n",
    "#         exclude = np.arange(X_trainval_quotient * (i-1), X_trainval_quotient * (i-1) + X_trainval_quotient, 1)\n",
    "#         X_val = X_trainval[(X_trainval_quotient * (i-1)) : (X_trainval_quotient * (i-1) + X_trainval_quotient)]\n",
    "#         Y_val = Y_trainval[(X_trainval_quotient * (i-1)) : (X_trainval_quotient * (i-1) + X_trainval_quotient)]\n",
    "#         print(exclude)\n",
    "#         print(X_trainval.shape)\n",
    "        \n",
    "    \n",
    "#         if i > 1:\n",
    "#             X_train = X_trainval[(X_trainval_quotient * (0)) : (X_trainval_quotient * (i-1) + X_trainval_quotient)]\n",
    "#             Y_train = Y_trainval[(X_trainval_quotient * (0)) : (X_trainval_quotient * (i-1) + X_trainval_quotient)]\n",
    "        \n",
    "#         if i < K:\n",
    "#             X_train.append(X_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (K-1) + X_trainval_rest)])\n",
    "#             X_train.append(Y_trainval[(X_trainval_quotient * (i)) : (X_trainval_quotient * (K-1) + X_trainval_rest)])\n",
    "            \n",
    "#         X_train = np.array(X_train)\n",
    "#         # X_train = np.delete(X_trainval, X_val, None)\n",
    "#         print(X_train.shape)\n",
    "#         # Y_train = np.delete(Y_trainval, exclude, None)\n",
    "        \n",
    "#     else:\n",
    "#         exclude = np.arange(X_trainval_quotient * (i-1), X_trainval_quotient * (i-1) + X_trainval_rest, 1)\n",
    "#         X_val = X_trainval[(X_trainval_quotient * (i-1)) : (X_trainval_quotient * (i-1) + X_trainval_rest)]\n",
    "#         Y_val = X_trainval[(X_trainval_quotient * (i-1)) : (X_trainval_quotient * (i-1) + X_trainval_rest)]\n",
    "        \n",
    "#         X_train = np.delete(X_trainval, exclude, None)\n",
    "        \n",
    "#         Y_train = np.delete(Y_trainval, exclude, None)\n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y_gt = Y_gt\n",
    "        self._train_step = None\n",
    "        self._loss = None\n",
    "        self._num_acc = None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.y_hat = None\n",
    "        self.config = config\n",
    "        \n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.            \n",
    " \n",
    "            init_W = tf.Variable(tf.zeros([X.shape[1], config.num_class]))\n",
    "            init_b = tf.Variable(tf.zeros([1, config.num_class]))\n",
    "\n",
    "\n",
    "            self.W = tf.get_variable(\"w\", initializer=init_W)\n",
    "            self.b = tf.get_variable(\"b\", initializer=init_b)\n",
    "            \n",
    "            self.y_hat = tf.nn.softmax(tf.matmul(self.X, self.W) + self.b)\n",
    "            \n",
    "            #X = tf.cast(X, tf.float32)\n",
    "            #W = tf.cast(W, tf.float32)\n",
    "            \n",
    "            #y_hat = tf.cast(y_hat,tf.float32)\n",
    "            #Y_gt = tf.cast(Y_gt,tf.float32)\n",
    "#             print(Y_gt[0:5])\n",
    "            #print(\"hello\")\n",
    "\n",
    "            \n",
    "#             print(X.shape)\n",
    "#             print(W.shape)\n",
    "#             #print(b.shape)\n",
    "#             print(y_hat.shape)\n",
    "#             print(Y_gt.shape)\n",
    "\n",
    "\n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            \n",
    "            #loss_temp = (-tf.reduce_sum(self.Y_gt * tf.log(self.y_hat), reduction_indices=[1]))\n",
    "            \n",
    "            #self._loss = (tf.reduce_mean(loss_temp + config.lambd*tf.nn.l2_loss(self.W)))\n",
    "            \n",
    "            self._loss = tf.reduce_mean(-tf.reduce_sum(self.Y_gt * tf.log(self.y_hat), reduction_indices=1)\n",
    "                                       + config.lambd*tf.nn.l2_loss(self.W))\n",
    "        \n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "\n",
    "        \n",
    "            # self._loss = (tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat, labels=np.transpose(Y_gt)) \n",
    "                                         #+ config.lambd*tf.nn.l2_loss(W)))\n",
    "            \n",
    "            \n",
    "\n",
    "            # self._loss = tf.reduce_mean(-tf.reduce_sum(Y_gt * tf.log(y_hat), reduction_indices=[1]))\n",
    "            \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            \n",
    "            \n",
    "            #TODO: Compute the accuracy\n",
    "            \n",
    "            #correct_prediction = tf.equal(tf.argmax(self.y_hat,1), tf.argmax(self.Y_gt,1))\n",
    "            self._num_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.y_hat,1), tf.argmax(self.Y_gt,1)), tf.float32))\n",
    "    \n",
    "    @property\n",
    "    def prediction(self):\n",
    "        \n",
    "        return self.y_hat\n",
    "    \n",
    "    @property\n",
    "    def train_op(self):\n",
    "\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        \n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        \n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "        \n",
    "    with tf.Session() as session: \n",
    "        \n",
    "        total_cost = sess.run(model.loss, feed_dict={model.X: X_test, model.Y_gt: Y_test})\n",
    "\n",
    "        accs = sess.run(model.num_acc, feed_dict={model.X: X_test, model.Y_gt: Y_test})\n",
    "    \n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    from keras.utils import np_utils\n",
    "\n",
    "    Y_train = np_utils.to_categorical(Y_train, 10)\n",
    "    Y_val = np_utils.to_categorical(Y_val, 10)\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement\n",
    "        \n",
    "            for ii, (x, y) in enumerate(get_batches(X_train, Y_train, config.batch_size), 1):\n",
    "                \n",
    "                sess.run(model.train_op, feed_dict={model.X: x, model.Y_gt: y})\n",
    "                \n",
    "                cost_train = sess.run(model.loss, feed_dict={model.X: x, model.Y_gt: y})\n",
    "\n",
    "                acc_train = sess.run(model.num_acc, feed_dict={model.X: x, model.Y_gt: y})\n",
    "#                 print(\"Cost Train: \", cost_train)\n",
    "#                 print(\"Acc Train: \", acc_train)\n",
    "                cost_trains.append(cost_train)\n",
    "                acc_trains.append(acc_train)\n",
    "            print(\"Epoch: %d :\" % (i + 1))\n",
    "            print(\"Train Loss: %f\" %  np.mean(cost_train))\n",
    "            print(\"Training acc: %f\" % np.mean(acc_train))\n",
    "\n",
    "            cost_val = sess.run(model.loss, feed_dict={model.X: X_val, model.Y_gt: Y_val})\n",
    "\n",
    "            acc_val = sess.run(model.num_acc, feed_dict={model.X: X_val, model.Y_gt: Y_val})\n",
    "\n",
    "            cost_vals.append(cost_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"Validation Loss: %f\" % cost_val)\n",
    "            print(\"Validation acc: %f\" % acc_val)\n",
    "            \n",
    "            X_train, Y_train = shuffle_train_data(X_train, Y_train)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 :\n",
      "Train Loss: 2.186741\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.199796\n",
      "Validation acc: 0.547050\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.179543\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.198277\n",
      "Validation acc: 0.672450\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.200089\n",
      "Training acc: 0.500000\n",
      "Validation Loss: 2.199304\n",
      "Validation acc: 0.590000\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.227756\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.200607\n",
      "Validation acc: 0.616850\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.143593\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.200726\n",
      "Validation acc: 0.579900\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.118073\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.202632\n",
      "Validation acc: 0.569050\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.185177\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.203767\n",
      "Validation acc: 0.537500\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.137339\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.201740\n",
      "Validation acc: 0.629600\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.203491\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.205620\n",
      "Validation acc: 0.613000\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.183597\n",
      "Training acc: 0.625000\n",
      "Validation Loss: 2.200509\n",
      "Validation acc: 0.625150\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.181042\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.201900\n",
      "Validation acc: 0.620400\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.147705\n",
      "Training acc: 0.812500\n",
      "Validation Loss: 2.199410\n",
      "Validation acc: 0.593500\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.225972\n",
      "Training acc: 0.500000\n",
      "Validation Loss: 2.200301\n",
      "Validation acc: 0.634300\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.280023\n",
      "Training acc: 0.500000\n",
      "Validation Loss: 2.205713\n",
      "Validation acc: 0.594000\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.262091\n",
      "Training acc: 0.375000\n",
      "Validation Loss: 2.199871\n",
      "Validation acc: 0.609800\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.151974\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.202470\n",
      "Validation acc: 0.639300\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.164862\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.202781\n",
      "Validation acc: 0.611400\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.170952\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.198454\n",
      "Validation acc: 0.656050\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.213040\n",
      "Training acc: 0.687500\n",
      "Validation Loss: 2.203502\n",
      "Validation acc: 0.622900\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.221835\n",
      "Training acc: 0.750000\n",
      "Validation Loss: 2.201841\n",
      "Validation acc: 0.620250\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.186848\n",
      "Training acc: 0.562500\n",
      "Validation Loss: 2.200804\n",
      "Validation acc: 0.541550\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "#lambda_list = [100]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.float32, [None, config.num_class])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "#         X = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "#         Y_gt = tf.placeholder(tf.float32, [None, config.num_class])\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        val_acc_list.append(acc_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    print(\"The validation accuracy for lambda %f is %f\" % (lambd, np.mean(val_acc_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lambd = 0.1 #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the impact of k in k-fold cross validation?\n",
    "\n",
    "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
    "\n",
    "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
    "\n",
    "4. Why does the loss increase, when the learning rate is too large?\n",
    "\n",
    "5. Do we apply L2-regularization for the bias $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "1) When K increases, a larger percent of the data is now considered in the training, so most of the structure of the data is taken in condseration when training, but if we have a very large K, then the validation set will be very small, so it won't be good enough to be a represtative for the data. It's much better to set K, so that the validation set will be about 20% of the whole data. \n",
    "2) When lamda increases, training will underfit as the penality term \"model complexity\" will contribute more to the cost function, while if lamda decreases, then training will overfit as the loss term will contribute more to the cost function, that's why we use k-fold cross validation to know which lambda will fit better to our problem. \n",
    "3) One reason is that performing gradient descent on all the data will be costly in terms of memory required, also this won't capture the full structure of the training data, as it's avaeraged on all the training samples, so it will tend to underfit. So small batches solve these problems, along with mitigating the effect of noisy samples. \n",
    "4) As large learning rate will make loss function to bounce around and diverge from the minima, this is what is called overshooting. \n",
    "5) No we don't as it doesn't contribute to the curvature of the model, it only controls the intercept part. So there is no point to regularize it to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "# global variable for training error each iteration \n",
    "cost_ite = []\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        self.w1 = np.random.normal(0, 0.1, n_features*n_hidden).reshape(n_features, n_hidden)\n",
    "        self.w2 = np.random.normal(0, 0.1, n_hidden*n_output).reshape(n_hidden, n_output)        \n",
    "        self.b1 = np.random.normal(0, 0.1, n_hidden)\n",
    "        self.b2 = np.random.normal(0, 0.1, n_output)\n",
    "        \n",
    "\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        #return 1 / (1 + np.exp(-z))\n",
    "        return expit(z)\n",
    "        \n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        return np.multiply(self.sigmoid(z), (1 - self.sigmoid(z)))\n",
    "\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "        ###stable_z = np.exp(z - max(z))\n",
    "        ###return stable_z/sum(stable_z)\n",
    "        ###ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
    "        \n",
    "        for i in range(z.shape[0]):\n",
    "            # for overflow: take z = z - max(z)\n",
    "            z[i] = np.exp(z[i] - np.amax(z))\n",
    "            \n",
    "            # derivative of the softmax : ...\n",
    "            \n",
    "            # for underflow: tkae log\n",
    "            #z[i] = np.log(z[i])-np.log(np.sum(z))\n",
    "            \n",
    "            # modified softmax: log(exp(z[i]-max(z)))-log(sum(exp(z[i]-max(z)))) \n",
    "        \n",
    "        #return np.random.rand(z.shape[0])\n",
    "        return z[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        #z2 = []\n",
    "        #a2 = []\n",
    "        #z3 = []\n",
    "        #a3 = []\n",
    "        #for x in X:\n",
    "            ##z2.append(np.add(np.matmul(x, self.w1.transpose()), self.b1))\n",
    "            ##a2.append(self.sigmoid(z2))\n",
    "            ##z3.append(np.add(np.matmul(x, self.w2.transpose()), self.b2))\n",
    "            ###a3.append(sigmoid(z3)) which changed to softmax\n",
    "            ##a3.append(softmax(z3))\n",
    "        \n",
    "        #self.w1.shape=(n_features, n_hidden)\n",
    "        #self.w2.shape=(n_hidden, n_output)\n",
    "        \n",
    "        z2 = np.add(np.matmul(X, self.w1), self.b1) #z2.shape=(n_feature, n_hidden)\n",
    "        a2 = self.sigmoid(z2) #a2.shape=(n_feature, n_hidden)\n",
    "        z3 = np.add(np.matmul(a2, self.w2), self.b2) #z3.shape=(n_feature, n_hidden)\n",
    "        a3 = self.softmax(z3) #a3.shape=(n_feature, n_hidden)\n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        #TODO Implement\n",
    "        ###X : array, shape = [n_samples, n_features]\n",
    "        ###return lambd/(n_samples * 2.0) * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        return (lambd/2.0) * (np.sum(np.square(self.w1)) + np.sum(np.square(self.w2)))\n",
    "        \n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement the cross-entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "        \n",
    "        epsilon: used to turn log(0) into log(epsilon)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        ### cost = 0.0\n",
    "        ### for i in range(len(self.n_output)):\n",
    "        ###     if (output[i] == 0):\n",
    "        ###         cost = cost - y_enc[i] * math.log(epsilon)\n",
    "        ###     else:\n",
    "        ###         cost = cost - (1 - y_enc[i]) * math.log(epsilon)\n",
    "        \n",
    "        \n",
    "        cost = 0.0\n",
    "        i = 0\n",
    "        while i < output.shape[0]:\n",
    "            if output[i] == 0:\n",
    "                cost += -(y_enc[i] * math.log(epsilon)) \n",
    "            else:\n",
    "                cost += -(y_enc[i] * math.log(output[i]))\n",
    "            i = i + 1\n",
    "            return cost\n",
    "    \n",
    "    \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, z3, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.                         #X.shape=(n_samples, n_features)\n",
    "        a2 : array, output of the hidden layer.   #a2.shape=(n_feature, n_hidden)\n",
    "        a3 : array, output of the output layer.   #a3.shape=(n_feature, n_hidden)\n",
    "        z2 : array, input of the hidden layer.    #z2.shape=(n_feature, n_hidden)\n",
    "        **z3 : array, input of output layer.      #z3.shape=(n_feature, n_hidden)\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        #self.w1.shape=(n_features, n_hidden)\n",
    "        #self.w2.shape=(n_hidden, n_output)\n",
    "        \n",
    "        #deravitive of L2 w.r.t a3 : a3 - y_enc \n",
    "        del3 = []\n",
    "        i = 0\n",
    "        for i in range(len(a3)):\n",
    "            del3.append(a3[i] - np.nonzero(y_enc[i])) #softmax gradient temporarily ditched: softmax_gradient(z3)*...\n",
    "            i += 1\n",
    "        del3 = np.asarray(del3) #del3.shape=a3.shape=(n_feature, n_hidden)\n",
    "        grad2 = np.dot(del3.transpose(), a2) #grad2.shape=(n_hidden, n_hidden)\n",
    "        del2 = self.sigmoid_gradient(z2)*np.dot(del3,self.w2) \n",
    "        #del2.shape=(n_feature, n_hidden)*(n_feature,n_output)\n",
    "        grad1 = np.dot(del2.transpose(), X) #grad1.shape=(n_feature, n_feature)\n",
    "        \n",
    "        grad4 = del3 \n",
    "        grad3 = del2\n",
    "       \n",
    "        return grad1, grad2, grad3, grad4\n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2,a2,z3,a3 = self.forward(X) \n",
    "        y_pred = np.argmax(a3,axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        \n",
    "        self.cost_ = []\n",
    "        num_batches = len(X_train) // self.batchsize\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # shuffle called each epoch\n",
    "            X_shuf, Y_shuf = self.shuffle_train_data(X_train, Y_train) \n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                \n",
    "                for b in range(num_batches):\n",
    "                    print(\"Batch:\",b) #com\n",
    "                    x_batch = X_shuf[b*self.batchsize:min(((b+1)*self.batchsize),len(X_train)),:]\n",
    "                    y_batch = Y_shuf[b*self.batchsize:min(((b+1)*self.batchsize),len(X_train))]\n",
    "                    print(\"Shape...\",y_batch.shape) #com \n",
    "        \n",
    "                # encode the labels\n",
    "                    y_enc = np.zeros((len(y_batch),self.n_output)) \n",
    "                    for i in range(len(y_batch)):\n",
    "                        y_enc[i][y_batch[i]] = 1 \n",
    "                    print(\"GroundTruth...\",y_enc[0]) #com\n",
    "                                       \n",
    "                # feedforward and loss computation\n",
    "                    z2,a2,z3,a3 = self.forward(x_batch)\n",
    "                    print(\"Preds:\",a3) #com\n",
    "                    pred_cost = self.loss(y_enc, a3)\n",
    "                    regularizer_cost = 0\n",
    "                    if(self.l2 != 0):\n",
    "                        regularizer_cost += self.L2_regularization(self.l2)\n",
    "                    cost = pred_cost + regularizer_cost\n",
    "                    # return cost/(no. of samples)\n",
    "                    cost = cost/y_enc.shape[0]\n",
    "                \n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                    grad1, grad2, grad3, grad4 = self.compute_gradient(x_batch, a2, a3, z2, z3, y_enc)\n",
    "                    if(self.l2 != 0):\n",
    "                        grad_2 += self.l2 * self.w2\n",
    "                        grad_1 += self.l2 * self.w1\n",
    "                \n",
    "                    self.w1 -= self.lr * grad1\n",
    "                    self.w2 -= self.lr * grad2\n",
    "                    self.b1 -= self.lr * grad3\n",
    "                    self.b2 -= self.lr * grad4\n",
    "                \n",
    "                \n",
    "         \n",
    "                # store the training errors for the follwing plotting \n",
    "                    cost_ite.append(cost)\n",
    "                \n",
    "            # update cost list after each epoch\n",
    "            y_enc_train = np.zeros((len(Y_train),self.n_output)) \n",
    "            for i in range(len(Y_train)):\n",
    "                y_enc[i][Y_train[i]] = 1\n",
    "            #return y_enc_train\n",
    "            \n",
    "            z2_train,a2_train,z3_train,a3_train = self.forward(X_train)\n",
    "            cost_train = self.loss(y_enc_train, a3_train) \n",
    "            self.cost_.append(cost_train)\n",
    "            print(self.cost_)\n",
    "                \n",
    "            \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $15.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features=X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=1000, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/1000\n",
      "Batch: 0\n",
      "Shape... (50,)\n",
      "GroundTruth... [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Preds: [0.26939082 0.34380946 0.3423715  0.57080902 0.2311594  0.13862902\n",
      " 0.45367552 0.20229013 0.18265486 0.24532588]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1,10) and (50,50) not aligned: 10 (dim 2) != 50 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-00dbb181338e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_trainval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-120-6f102173538f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, Y_train, verbose)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# compute gradient via backpropagation and update the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     \u001b[0mgrad1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m                     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                         \u001b[0mgrad_2\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-120-6f102173538f>\u001b[0m in \u001b[0;36mcompute_gradient\u001b[1;34m(self, X, a2, a3, z2, z3, y_enc)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mdel3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdel3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#del3.shape=a3.shape=(n_feature, n_hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mgrad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#grad2.shape=(n_hidden, n_hidden)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[0mdel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdel3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m#del2.shape=(n_feature, n_hidden)*(n_feature,n_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,1,10) and (50,50) not aligned: 10 (dim 2) != 50 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (50000,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-85f14563570d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnum_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchsize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_ite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training Error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3356\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3357\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3358\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3359\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1527\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'plot'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[1;32m--> 242\u001b[1;33m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[0;32m    243\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50000,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "num_iters = nn.batchsize * nn.epochs \n",
    "plt.plot(range(0,num_iters), cost_ite)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error vs Iterations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "#print (len(nn.cost_)) \n",
    "plt.plot(range(1,nn.epochs+1), nn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training Error vs Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
